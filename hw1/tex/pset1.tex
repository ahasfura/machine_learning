\documentclass{article}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{mathtools}
\usepackage{fullpage}

\begin{document}
\title{6.867: Homework 1}
\author{Andres Hasfura and Kathryn Evans}
\maketitle
\section{Gradient Descent}
Gradient descent works by updating an initial guess by taking a step opposite the direction of the gradient at that initial guess. Moving opposite the gradient helps  to effectively move "downhill" in terms of the function value.  The formulation of the gradient descent method is as follows
\begin{equation}
x(\tau+1) = x(\tau) - \alpha \nabla F(x(\tau))
\end{equation}

Where $ x(\tau)$ is the current guess, with $\tau$ being the current iteration, $\alpha$ is the step size and $F(x)$ is the function which will be evaluated. This method is carried out until there is convergence, which is checked by looking at the norm of successive guesses and seeing if it is less than the required tolerance, $\epsilon$,   


\begin{equation} 
\left \lVert x(\tau+1)-x(\tau) \right \rVert \leq \epsilon
\end{equation}



\subsection*{ Gradient Descent Benchmarking}
katy stuff!

\subsection*{ Central Differences}
Here are the results from testing the two methods at different points.

\begin{center}
  \begin{tabular}{ | c | c | c | c | c | }
    \hline
     point & function & analytically & numerically & difference \\ \hline
     (11.2) & $f(x) = x^2$ & 22.4 & 22.4000 & $3.7*10^{-13}$ \\ \hline
     (1, 1) & $f(x) = x_1^2+ x_2^2$ & (2, 2) & (2.0000, 2.0000) & $(10^{-15}, 10^{-15})$ \\ \hline
     (3) & $f(x) = sin(x)$ & -.9900 & -.9900 & $4.1*10^{-10}$ \\ 
    \hline
  \end{tabular}
\end{center}

The analytical and numerical approximation for the gradient remain very close.

\end{document}