\documentclass[10pt,twocolumn]{article}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{mathtools}
\usepackage{fullpage}
\usepackage{float}
\usepackage{color, soul}

\begin{document}
\title{6.867: Homework 1}

\section{Gradient Descent}
Gradient descent works by updating an initial guess by taking a step opposite the direction of the gradient at that initial guess. Moving opposite the gradient helps  to effectively move "downhill" in terms of the function value.  The formulation of the gradient descent method is as follows
\begin{equation}
x(\tau+1) = x(\tau) - \alpha \nabla F(x(\tau))
\end{equation}

Where $ x(\tau)$ is the current guess, with $\tau$ being the current iteration, $\alpha$ is the step size and $F(x)$ is the function which will be evaluated. This method is carried out until there is convergence, which is checked by looking at the norm of successive guesses and seeing if it is less than the required tolerance, $\epsilon$,   



\begin{equation} 
\left \lVert x(\tau+1)-x(\tau) \right \rVert \leq \epsilon
\end{equation}

This is implemented in the code in a way that could be tested on N-D functions. 



\subsection*{ Gradient Descent Testing}

We implemented this code on a few different functions, first simply a 1-D quadratic bowl, then on a N-D quadratic bowl. In these instances the gradient descent performed well, finding the true solution with ease.  
\begin{figure}[H]
\center
\includegraphics[scale =.4]{2DQuadBowl.pdf}
\caption{Gradient Descent Program located the minimum, indicated by the red dot, on 2D quadratic Bowl}
\end{figure}


By implementing the gradient descent code on a sine wave, which has multiple minima, the effects of input parameters on the declared minima could be investigated. The figures below show the effect of changing the initial guess and the tolerance. 
\begin{figure}[H]
\center
\includegraphics[scale=.3]{1Dsin.pdf}
	

\includegraphics[scale=.3]{1DsinHighTol.pdf}
	
\includegraphics[scale=.3]{1DsinStepSize.pdf}
\caption{ Effect of changing input parameters. a) shows the effect of different initial guess leads to a different minimum. b) Setting tolerances too large does not capture true minimum. c) Setting step size too small increases number of iterations. Setting it too large also increases number of iterations by overshooting the minimum}
\end{figure}
By changing the initial guess at points where the slopes head towards different minima of course changes the output of the gradient descent process. If the tolerance is too high, the solver converges too quickly and does not find the true minima.  Changing the step size caused an increase in computation time because if it is excessively small, the number of steps increases dramatically and it if it is too large the solver can overshoot the minima. 

\subsection*{ Central Differences}
Here are the results from testing the two methods at different points.

\begin{center}
  \begin{tabular}{ | c | c | c | c | c | }
    \hline
     point & function  & diff \\ \hline
     (11.2) & $f(x) = x^2$ & $3.7*10^{-13}$ \\ \hline
     (1, 1) & $f(x) = x_1^2+ x_2^2$ & $(10^{-15}, 10^{-15})$ \\ \hline
     (3) & $f(x) = sin(x)$ & $4.1*10^{-10}$ \\ 
    \hline
  \end{tabular}
\end{center}

The analytical and numerical approximation for the gradient remain very close.

\subsection*{Comparison to Matlab solver}

To better evaluate the effectiveness of our solver we compared it to the performance of the Matlab unconstrained optimization solver, fminunc. We chose a trickier function, with local minima to compare performance, 

\begin{equation}
f(x)= 3x^4-8x^3+6x^2+17
\end{equation}

\begin{figure}[H]
\label{fig: fminunc}
\center
\includegraphics[scale =.6]{comparetofminunc.pdf}
\caption{Results of gradient descent algorithm versus results of fminunc for two initial guesses. For initial guess, $x_i= 1.5$ the gradient descent algorithm finds a local minimum but fminunc finds the global minimum}
\end{figure}
 

\begin{center}
  \begin{tabular}{ | c | c | c | c | c | }
    \hline
     function & $x_i$ & calls &  time (s) & global \\ \hline
     gradient descent & .5 &  11 &  0.009 & YES \\ \hline
     gradient descent & 1.5 & 1274  & 0.010 & NO \\ \hline
    fminunc & .5  & 6 & 0.022 & YES  \\ \hline
      fminunc &1.5 & 6 & 0.027 & YES \\ 
    \hline
  \end{tabular}
\end{center}
Depending on the starting guess, fminunc and our gradient descent algorithm sometimes differ even with the same tolerances applied. As shown in the above figure and table, fminunc is better than our gradient descent algorithm at finding the global minimum in the presence of local minima.  However, as fminunc takes more precise, and therefore fewer, steps,  it also requires more computation time .

\section{Linear Basis Function Regression}

\subsection*{Maximum Likelihood Weights}

The procedure for computing the maximum likelihood weights is as follows. First we must take the vector of inputs $x$ and compute the each individual basis vector, $\phi$, according to the definition given by the problem set, $\phi_1(x) = x, ... , \phi_M(x) = x^M$. Then we concatenate the basis vectors into a matrix $\Phi$. Finally, we can directly solve for the weight vector, $w$, using the formula $w_{ml} = (\Phi^T  \Phi)^{-1}  \Phi^T y$.

Shown below is a comparison of the weights we calculated versus the weights Bishop calculated for the same eight data points and different values of M and the corresponding graphs of the fit. Overall our results are nearly the same as those from the textbook.

\begin{table}[H]
\footnotesize
\begin{center}
	\begin{tabular}{| c | c | c | c | c |}
	\hline
	Source & M=0 & M=1 & M=3 & M=9 \\ \hline
	Computed  &  .19 & $\begin{bmatrix} .82 \\ -1.27 \end{bmatrix}$ & $\begin{bmatrix} .31 \\ 7.99 \\ -25.43 \\ 17.37 \end{bmatrix}$ & $\begin{bmatrix} .35 \\ \textcolor{red}{231.73} \\ \textcolor{red}{-5307.04} \\ \textcolor{red}{48435.64} \\  \textcolor{red}{-231018.45} \\ \textcolor{red}{638357.05} \\ \textcolor{red}{-1059050.39} \\ \textcolor{red}{1039740.82} \\ \textcolor{red}{-556279.81} \\ \textcolor{red}{124890.36} \end{bmatrix}$  \\ \hline
	Bishop  &  .19 & $\begin{bmatrix} .82 \\ -1.27 \end{bmatrix}$ & $\begin{bmatrix} .31 \\ 7.99 \\ -25.43 \\ 17.37 \end{bmatrix}$ & $\begin{bmatrix} .35 \\ 232.37 \\ -5321.83 \\ 48568.31 \\  -231639.30 \\ 640042.26 \\ -1061800.52 \\ 1042400.18 \\ -557682.99 \\ 125201.43 \end{bmatrix}$  \\ \hline
	\end{tabular}
\caption{Computed weights versus weights given in Bishop. Values in red show discrepancies.}
\end{center}
\end{table}



\begin{figure}[H]
\center
\includegraphics[scale =.45]{ML_weight.pdf}
\caption{Curves fit to data by maximum likelihood weights}
\end{figure}

\subsection*{ Sum of Squares Error}
For a particular hypothesis, $w, M $ and data points $(X, Y)$ the sum of squares error and its derivative are given by

\begin{align}
SSE(w) = (\Phi(M, X) w - Y)^T(\Phi(M, X) w - Y)\\
\frac{\partial SSE}{\partial w}= 2 \Phi(M,X)(\Phi(M,X)w -Y) 
\end{align}

The derivative was verified by testing against the numerical derivative code. Discrepancies for all tried values were under $10^{-10}$. 

For the same tolerances fminunc and the gradient descent algorithm found the same minimum. This is because the objective function is effectively a n-dimesion quadratic bowl so it has no issues with local minima. However, the gradient descent function takes significantly more function calls than fminunc. The exact number depends on the chosen step size and initial guess but can be many orders of magnitude larger. 

\subsection*{ Sine Basis Function}
If the basis functions were sine functions, it would fit this data set very well, just because the data was generated from a sine function. The weight for the $sin(2 \pi x)$ will be large, whereas the weights for the other larger frequency sine terms will be close to zero.

Had we not known the underlying distribution was itself a sine function, using sine functions for your basis functions can have two main potential disadvantages. First, it can require many terms to model simple functions (such as a line), and second not every function can be modeled only by sines, only odd functions can be modeled.

\section{Ridge Regression}

\subsection*{ Implementation}

To implement ridge regression a simple modification to the equation used in problem 2 was made to both the weight selection and the quadratic regularization applied to the error function 

\begin{equation}
E(w_{r}) = \frac{1}{2}((\Phi w_{r}  -Y)^T( \Phi w_{r}  -Y) + \lambda w_{r}^T w_{r}))
\end{equation}
Exhibiting the closed form solution, 
\begin{equation}
w_{ridge} = (\lambda I + \Phi^T  \Phi)^{-1}  \Phi^T y
\end{equation}

To compare to Bishop we implemented and plotted the results for $\lambda = {0, .00001, .1} $

\begin{figure}[H]
\center
\includegraphics[scale =.5]{rr_lambdas.png}
\caption{Effect of $\lambda$ on data fit}
\end{figure}

Large $\lambda$ values penalize higher order terms. With M fixed, $\lambda$ now controls the effective complexity
of the model and hence determines the degree of over-fitting.  $\lambda = 0$ is the same as the maximum likelihood solution, so it is reasonable for lower order fits and overfitted when M is large. $\lambda = .1$ causes the polynomial to be too smooth, so the error to be very high. $\lambda = .00001$ strikes a balance between too much error while still being generalizable. $\lambda = .00001$ and M = 9 still produces a curve that looks similar to $sin(2\pi x)$, the function we originally sampled from.

\subsection*{ Model Selection}
In this problem we were provide the following three datasets: train, test and validation. 
\begin{figure}[H]
\center
\includegraphics[scale =.4]{test_train_validate.png}
\caption{Provided data. Looking at the overall data the trend appears roughly linear. However there is a large outlier in the training data}
\end{figure}

From the above data we can infer issues we may come across. The training data set has a single outlier that does not follow the generally linear pattern which will influence the preferred model order. 

Given the above training, test, and validation data, we ran a grid search optimization to find the best $\lambda$ and M for the ridge regression weights. The best model parameters were those which minimized the sums of squares error. Specifically we searched across $M= {0,1,2,3,4,5}$  for $10^{-10} < \lambda <10 $. Below are the results for the error cost at the optimal lambda given set M on the validation data set when trained on the training set. 

\begin{table}[H]
\begin{center}
  \begin{tabular}{ | c | c | c | c | c | }
    \hline
     M & $\lambda ^*$  &$ E(w^*)$ \\ \hline
     0 & 10 & 21.92  \\ \hline
     1 & 5.99 & 11.46   \\ \hline
     2 &1.67 & 8.92   \\ \hline
     3 & .1 & 3.60   \\ \hline
     4 & .77 & \textcolor{red}{0.99}   \\ \hline
     5 & 10 & 2.25  \\ \hline    
    \hline
  \end{tabular}
  \caption{Ridge Regression:  Sum of Squares error at the optimal lambda for validation data sets for given order. Optimal model selection given just training and validation data is highlighted in red. }
\end{center}
\label{table:ave_sse}
\end{table}



The minimum sum of squares error for the validation data set occurred at M = 4 and $\lambda = .77$. This claims that the best fit is a 4th order polynomial which looks like it somewhat fits the validation data .
\begin{figure}[H]
\center
\includegraphics[scale =.5]{modelcomparison.png}
\caption{Ridge Regression fit from training data optimized on validation data shown with the original validation data. Shown are the solutions for $M=1-4$. The best fit was determined to be a 4th order polynomial.}
\end{figure}

However if we look at this fit compared to the test data, the test data looks a lot more linear that the validation data. This causes the model to not fit as well at the lower extreme.  Due to the outlier in the training data and the small cluster at the lower extreme of the validation data that looks flat, the optimal solution is a 4th order polynomial. Had we optimized over the more linear looking test data, we probably would have seen a linear model as the solution. 

\begin{figure}[H]
\center
\includegraphics[scale =.4]{testplot.png}
\caption{Ridge Regression fit from training data optimized on validation data against the test data. The model does not fit well at the lower x values as the test data looks linear }
\end{figure}

 There is an observed trade off between the model order, M and the corresponding $\lambda$ because both of them influence the amount of  complexity of the model and corresponding  degree of over-fitting.  If we wanted to further optimize the model parameters an exhaustive cross validation might provide different insight.  Since the outlier in the training data would no longer be the only training set,it would not influence the weights, w as strongly as it did here.  

\subsection*{ BlogFeedback Dataset}
For this data set we will implement a simple linear regression. For a data entry x, where x is defined by D attributes, $\Phi(x)= \begin{bmatrix} 1 &  x_1 & ... & x_D \end{bmatrix}$ and $y = w^T x$. With this specification we only have to worry about the effect of one parameter, $\lambda$. 

Again we implemented a gird search for the minimum $\lambda$ by considering logarithmically spaced values between $10^{-10}$ and $10^4$. The corresponding cost is shown in Fig. 7. The value of $\lambda $ that gave us the lowest cost was found to be $\lambda ^* = 3.51 \cdot 10^3 $ . 
\begin{figure}[H]
\center
\includegraphics[scale =.4]{3_3.png}
\caption{Ridge Regression: Effect of choice of $\lambda$ on the error of the validation data.  The minimum is indicated by the red circle}
\end{figure}


Despite optimizing the parameter $\lambda$ to achieve the smallest possible error,  the error is extremely large. It makes sense to be large because it it not normalized my the number of data entries. However dividing by N, still leaves this model with a MSE of approximately $10^3$. 

\section{Generalizations}
\subsection*{Outlier and LAD}

Using the same test set as the ridge regression question, we implemented a LAD approach, which includes a linear term in error but a quadratic term in the regularization, 

\begin{equation}
E(w_{LAD}) = \frac{1}{2}(|\Phi w_{LAD}  -Y| + \lambda w_{LAD}^T w_{LAD}))
\end{equation}


Because there is no close form calculation for the weights in LAD, we instead use a gradient descent algorithm to select the weights that minimize the LAD error on the training data. 

From a numerical stand point, one important thing to note is that when implementing LAD, we introduce a nonlinearity into the optimization objective function with the absolute value function. This gets translated to a non-differentiability in the gradient. Therefore to get convergence in the gradient descent algorithm we must implement the sub gradient method in which the step size continuously decays by 

\begin{equation}
\alpha= \frac{\alpha_0 }{\sqrt{n}}
\end{equation}

For all $(M, \lambda)$ the gradient search parameters  that we found to be  convergence : 
\begin{align}
w_i =.1 \cdot ones(1, M+1) \\
\alpha_0 = .001 \\
\epsilon = 10^{-4} \\
h = .1 \\ 
\end{align}
Tweaking these too much lead to slow convergence or no convergence. 

This process is implement the gradient descent on $w$ on the LAD error with regularization on the training data at each point within the grid search across M, and lambda. Then find the grid point that minimizes the absolute value of the error of the validation data. The grid search included values of $M= {0,1,2,3,4}$  for $10^{-10} < \lambda <10 $. The results are shown in  Table 3. The optimal model, ($M=1, \lambda = 0$) was very different to the one chosen with ridge regression, ($M=4, \lambda=.77$) .
\begin{table}
\begin{center}
  \begin{tabular}{ | c | c | c | c | c | }
    \hline
     M & $\lambda ^*$  & E($w^*$) \\ \hline
     0 & 10 & 12.71  \\ \hline
     1 & $ 10^{-10}$ & \textcolor{red}{2.24}   \\ \hline
     2 & 1 & 9.63   \\ \hline
     3 & 2.78 & 5.88   \\ \hline
     4 & $10^{-10}$ & 7.46   \\ \hline
    
    \hline
  \end{tabular}
  \caption{LAD Regression: Error at the optimal lambda for validation data sets for given order. Optimal model selection given just training and validation data is highlighted in red. }
\end{center}
\label{table:ave_sse}
\end{table}






For the optimal model, ($M=1, \lambda = 0$), the optimal weights are $w= \begin{bmatrix} 0.8919  &&   0.9015 \end{bmatrix}$ and the prediction fit is shown in Fig. 10. 
\begin{figure}[H]
\center
\includegraphics[scale =.4]{4_1.png}
\caption{Model fit for LAD regression pictured with validation data}
\end{figure}

This result fits much better and makes more sense when the test data is overlaid. 
\begin{figure}[H]
\center
\includegraphics[scale =.4]{4_1test.png}
\caption{Model fit for LAD regression pictured with test data as well}
\end{figure}

Utilizing the l1 norm turned out to be more robust that using SSE as we were able to find the linear solution with LAD and not with Ridge. This is because large outliers that would produce errors greater than one are not further increased by squaring. Therefore LAD might be a good choice if the data set is known to have an extreme outlier. However LAD does not have a close form solution and requires an approximation of the gradient to calculate the weights which could be computationally expensive and could have troubles with convergence. 
 
 
\end{document}
